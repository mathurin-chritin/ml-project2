{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "This notebook uses a simple logistic regression to make a very basic prediction on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = \"\"\n",
    "\n",
    "EMBEDDINGS = f\"tweet_embeddings/embeddings{SUFFIX}.txt\"\n",
    "VOCAB = f\"tweet_embeddings/vocab{SUFFIX}.txt\"\n",
    "\n",
    "POS_TWEETS = f\"twitter-datasets/train_pos{SUFFIX}.txt\"\n",
    "NEG_TWEETS = f\"twitter-datasets/train_neg{SUFFIX}.txt\"\n",
    "TEST_DATA = f\"twitter-datasets/test_data{SUFFIX}.txt\"\n",
    "\n",
    "# parse embeddings\n",
    "vecs = {}\n",
    "with open(EMBEDDINGS, \"r\") as f:\n",
    "    for line in f:\n",
    "        pline = line.rstrip().split(' ')\n",
    "        word = pline[0]\n",
    "        vecs[word] = np.array([float(x) for x in pline[1:]])\n",
    "\n",
    "# parse vocabulary and build an index\n",
    "with open(VOCAB, \"r\") as f:\n",
    "    vocab = {x.rstrip().split(' ')[0]: i for i,x in enumerate(f)}\n",
    "\n",
    "embeddings = np.zeros((len(vocab), len(vecs[list(vecs.keys())[0]])))\n",
    "for w, v in vecs.items():\n",
    "    if w == \"<unk>\":\n",
    "        continue\n",
    "    embeddings[vocab[w], :] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NEG_TWEETS, \"r\") as f:\n",
    "    n_tweets = [line.rstrip().split() for line in f]\n",
    "with open(POS_TWEETS, \"r\") as f:\n",
    "    p_tweets = [line.rstrip().split() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_tweets = []\n",
    "testing_tweets_ids = []\n",
    "with open(TEST_DATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        parsed_line = line.rstrip().split(',')\n",
    "        testing_tweets.append(','.join(parsed_line[1:]).split())\n",
    "        testing_tweets_ids.append(int(parsed_line[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets...\n",
      "0/100000 (0 %)\n",
      "1000/100000 (1 %)\n",
      "2000/100000 (2 %)\n",
      "3000/100000 (3 %)\n",
      "4000/100000 (4 %)\n",
      "5000/100000 (5 %)\n",
      "6000/100000 (6 %)\n",
      "7000/100000 (7 %)\n",
      "8000/100000 (8 %)\n",
      "9000/100000 (9 %)\n",
      "10000/100000 (10 %)\n",
      "11000/100000 (11 %)\n",
      "12000/100000 (12 %)\n",
      "13000/100000 (13 %)\n",
      "14000/100000 (14 %)\n",
      "15000/100000 (15 %)\n",
      "16000/100000 (16 %)\n",
      "17000/100000 (17 %)\n",
      "18000/100000 (18 %)\n",
      "19000/100000 (19 %)\n",
      "20000/100000 (20 %)\n",
      "21000/100000 (21 %)\n",
      "22000/100000 (22 %)\n",
      "23000/100000 (23 %)\n",
      "24000/100000 (24 %)\n",
      "25000/100000 (25 %)\n",
      "26000/100000 (26 %)\n",
      "27000/100000 (27 %)\n",
      "28000/100000 (28 %)\n",
      "29000/100000 (28 %)\n",
      "30000/100000 (30 %)\n",
      "31000/100000 (31 %)\n",
      "32000/100000 (32 %)\n",
      "33000/100000 (33 %)\n",
      "34000/100000 (34 %)\n",
      "35000/100000 (35 %)\n",
      "36000/100000 (36 %)\n",
      "37000/100000 (37 %)\n",
      "38000/100000 (38 %)\n",
      "39000/100000 (39 %)\n",
      "40000/100000 (40 %)\n",
      "41000/100000 (41 %)\n",
      "42000/100000 (42 %)\n",
      "43000/100000 (43 %)\n",
      "44000/100000 (44 %)\n",
      "45000/100000 (45 %)\n",
      "46000/100000 (46 %)\n",
      "47000/100000 (47 %)\n",
      "48000/100000 (48 %)\n",
      "49000/100000 (49 %)\n",
      "50000/100000 (50 %)\n",
      "51000/100000 (51 %)\n",
      "52000/100000 (52 %)\n",
      "53000/100000 (53 %)\n",
      "54000/100000 (54 %)\n",
      "55000/100000 (55 %)\n",
      "56000/100000 (56 %)\n",
      "57000/100000 (56 %)\n",
      "58000/100000 (57 %)\n",
      "59000/100000 (59 %)\n",
      "60000/100000 (60 %)\n",
      "61000/100000 (61 %)\n",
      "62000/100000 (62 %)\n",
      "63000/100000 (63 %)\n",
      "64000/100000 (64 %)\n",
      "65000/100000 (65 %)\n",
      "66000/100000 (66 %)\n",
      "67000/100000 (67 %)\n",
      "68000/100000 (68 %)\n",
      "69000/100000 (69 %)\n",
      "70000/100000 (70 %)\n",
      "71000/100000 (71 %)\n",
      "72000/100000 (72 %)\n",
      "73000/100000 (73 %)\n",
      "74000/100000 (74 %)\n",
      "75000/100000 (75 %)\n",
      "76000/100000 (76 %)\n",
      "77000/100000 (77 %)\n",
      "78000/100000 (78 %)\n",
      "79000/100000 (79 %)\n",
      "80000/100000 (80 %)\n",
      "81000/100000 (81 %)\n",
      "82000/100000 (82 %)\n",
      "83000/100000 (83 %)\n",
      "84000/100000 (84 %)\n",
      "85000/100000 (85 %)\n",
      "86000/100000 (86 %)\n",
      "87000/100000 (87 %)\n",
      "88000/100000 (88 %)\n",
      "89000/100000 (89 %)\n",
      "90000/100000 (90 %)\n",
      "91000/100000 (91 %)\n",
      "92000/100000 (92 %)\n",
      "93000/100000 (93 %)\n",
      "94000/100000 (94 %)\n",
      "95000/100000 (95 %)\n",
      "96000/100000 (96 %)\n",
      "97000/100000 (97 %)\n",
      "98000/100000 (98 %)\n",
      "99000/100000 (99 %)\n",
      "Loading tweets...\n",
      "0/100000 (0 %)\n",
      "1000/100000 (1 %)\n",
      "2000/100000 (2 %)\n",
      "3000/100000 (3 %)\n",
      "4000/100000 (4 %)\n",
      "5000/100000 (5 %)\n",
      "6000/100000 (6 %)\n",
      "7000/100000 (7 %)\n",
      "8000/100000 (8 %)\n",
      "9000/100000 (9 %)\n",
      "10000/100000 (10 %)\n",
      "11000/100000 (11 %)\n",
      "12000/100000 (12 %)\n",
      "13000/100000 (13 %)\n",
      "14000/100000 (14 %)\n",
      "15000/100000 (15 %)\n",
      "16000/100000 (16 %)\n",
      "17000/100000 (17 %)\n",
      "18000/100000 (18 %)\n",
      "19000/100000 (19 %)\n",
      "20000/100000 (20 %)\n",
      "21000/100000 (21 %)\n",
      "22000/100000 (22 %)\n",
      "23000/100000 (23 %)\n",
      "24000/100000 (24 %)\n",
      "25000/100000 (25 %)\n",
      "26000/100000 (26 %)\n",
      "27000/100000 (27 %)\n",
      "28000/100000 (28 %)\n",
      "29000/100000 (28 %)\n",
      "30000/100000 (30 %)\n",
      "31000/100000 (31 %)\n",
      "32000/100000 (32 %)\n",
      "33000/100000 (33 %)\n",
      "34000/100000 (34 %)\n",
      "35000/100000 (35 %)\n",
      "36000/100000 (36 %)\n",
      "37000/100000 (37 %)\n",
      "38000/100000 (38 %)\n",
      "39000/100000 (39 %)\n",
      "40000/100000 (40 %)\n",
      "41000/100000 (41 %)\n",
      "42000/100000 (42 %)\n",
      "43000/100000 (43 %)\n",
      "44000/100000 (44 %)\n",
      "45000/100000 (45 %)\n",
      "46000/100000 (46 %)\n",
      "47000/100000 (47 %)\n",
      "48000/100000 (48 %)\n",
      "49000/100000 (49 %)\n",
      "50000/100000 (50 %)\n",
      "51000/100000 (51 %)\n",
      "52000/100000 (52 %)\n",
      "53000/100000 (53 %)\n",
      "54000/100000 (54 %)\n",
      "55000/100000 (55 %)\n",
      "56000/100000 (56 %)\n",
      "57000/100000 (56 %)\n",
      "58000/100000 (57 %)\n",
      "59000/100000 (59 %)\n",
      "60000/100000 (60 %)\n",
      "61000/100000 (61 %)\n",
      "62000/100000 (62 %)\n",
      "63000/100000 (63 %)\n",
      "64000/100000 (64 %)\n",
      "65000/100000 (65 %)\n",
      "66000/100000 (66 %)\n",
      "67000/100000 (67 %)\n",
      "68000/100000 (68 %)\n",
      "69000/100000 (69 %)\n",
      "70000/100000 (70 %)\n",
      "71000/100000 (71 %)\n",
      "72000/100000 (72 %)\n",
      "73000/100000 (73 %)\n",
      "74000/100000 (74 %)\n",
      "75000/100000 (75 %)\n",
      "76000/100000 (76 %)\n",
      "77000/100000 (77 %)\n",
      "78000/100000 (78 %)\n",
      "79000/100000 (79 %)\n",
      "80000/100000 (80 %)\n",
      "81000/100000 (81 %)\n",
      "82000/100000 (82 %)\n",
      "83000/100000 (83 %)\n",
      "84000/100000 (84 %)\n",
      "85000/100000 (85 %)\n",
      "86000/100000 (86 %)\n",
      "87000/100000 (87 %)\n",
      "88000/100000 (88 %)\n",
      "89000/100000 (89 %)\n",
      "90000/100000 (90 %)\n",
      "91000/100000 (91 %)\n",
      "92000/100000 (92 %)\n",
      "93000/100000 (93 %)\n",
      "94000/100000 (94 %)\n",
      "95000/100000 (95 %)\n",
      "96000/100000 (96 %)\n",
      "97000/100000 (97 %)\n",
      "98000/100000 (98 %)\n",
      "99000/100000 (99 %)\n",
      "Loading tweets...\n",
      "0/10000 (0 %)\n",
      "1000/10000 (10 %)\n",
      "2000/10000 (20 %)\n",
      "3000/10000 (30 %)\n",
      "4000/10000 (40 %)\n",
      "5000/10000 (50 %)\n",
      "6000/10000 (60 %)\n",
      "7000/10000 (70 %)\n",
      "8000/10000 (80 %)\n",
      "9000/10000 (90 %)\n"
     ]
    }
   ],
   "source": [
    "# convert a tweet to an embedding of shape (20,) which is the mean of each embedding of each word.\n",
    "series_train = []\n",
    "series_test = []\n",
    "\n",
    "def load_tweets(tweets_list, series, label=None):\n",
    "    print(\"Loading tweets...\")\n",
    "    i = 0\n",
    "    tot = len(tweets_list)\n",
    "    for tweet in tweets_list:\n",
    "        if i%1000 == 0:\n",
    "            print(f\"{i}/{tot} ({int(i/tot*100)} %)\")\n",
    "        indices = [vocab.get(word, -1) for word in tweet if word in vocab.keys()]\n",
    "        if len(indices) == 0:\n",
    "            tweet_embedding = np.zeros((20,))\n",
    "        else:\n",
    "            tweet_embedding = np.mean(embeddings[indices], axis=0)\n",
    "        serie_dict = {f'f{x+1}': data for x, data in enumerate(tweet_embedding)}\n",
    "        if label is not None:\n",
    "            serie_dict['label'] = label\n",
    "        series.append(pd.Series(serie_dict))\n",
    "        i+=1\n",
    "    return series\n",
    "\n",
    "# add both negative and positive tweets, will be shuffled later\n",
    "series_train = load_tweets(p_tweets, series_train, 1)\n",
    "series_train = load_tweets(n_tweets, series_train, -1)\n",
    "\n",
    "# no label since this is the prediction set\n",
    "series_test = load_tweets(testing_tweets, series_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame...\n",
      "Training DataFrame sample\n",
      "        f17       f18       f19       f20  label\n",
      "0  0.006545  0.592248  0.016852 -1.591182    1.0\n",
      "1 -0.261796  0.292175 -0.168170 -1.485552    1.0\n",
      "2 -0.408993  0.321010  0.124718 -1.531494    1.0\n",
      "3  0.148516  0.494902 -0.254856 -1.533331    1.0\n",
      "4 -0.404575  0.489780 -0.030524 -1.233568    1.0\n",
      "Testing DataFrame sample\n",
      "            f16       f17       f18       f19       f20\n",
      "index                                                  \n",
      "1     -0.299459 -0.379505  0.444008  0.241766 -1.407697\n",
      "2     -0.475540  0.139808  0.363823 -0.054524 -2.219004\n",
      "3     -0.233857  0.135580 -0.039838 -0.145392 -1.633895\n",
      "4     -0.152084  0.272331  0.429447 -0.185306 -1.995526\n",
      "5     -0.001477  0.108543 -0.007657 -0.152091 -1.633170\n"
     ]
    }
   ],
   "source": [
    "# use DataFrames to represent data\n",
    "print(\"Creating DataFrame...\")\n",
    "df_train = pd.DataFrame(series_train)\n",
    "\n",
    "df_test = pd.DataFrame(series_test)\n",
    "df_test[\"index\"] = testing_tweets_ids\n",
    "df_test.set_index(['index'], inplace=True) # keep indexes as in the input file\n",
    "\n",
    "\n",
    "# print last 5 columns of the DataFrames (df_test has no \"label\" column)\n",
    "print(\"Training DataFrame sample\")\n",
    "print(df_train[df_train.columns[-5:]].head(5))\n",
    "print(\"Testing DataFrame sample\")\n",
    "print(df_test[df_test.columns[-5:]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1234\n",
    "# shuffle the dataframe\n",
    "df_train = df_train.sample(n=df_train.shape[0], random_state=RANDOM_SEED)\n",
    "X = df_train[df_train.columns[:-1]]\n",
    "y = df_train[df_train.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED)\n",
    "# X_train.to_csv(\"X_train.csv\", index=False)\n",
    "# X_test.to_csv(\"X_test.csv\", index=False)\n",
    "# y_train.to_csv(\"y_train.csv\", index=False)\n",
    "# y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7175030680951274\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(penalty='l2', random_state=RANDOM_SEED+2, max_iter=100)\n",
    "logistic.fit(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions on the testing set\n",
    "predictions = logistic.predict(df_test)\n",
    "df_predictions = pd.DataFrame({\"Id\": df_test.index,\n",
    "                               \"Prediction\": predictions},\n",
    "                               dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to predictions/very_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "prediction_file = \"predictions/very_baseline.csv\"\n",
    "print(f\"Saving to {prediction_file}\")\n",
    "df_predictions.to_csv(prediction_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
